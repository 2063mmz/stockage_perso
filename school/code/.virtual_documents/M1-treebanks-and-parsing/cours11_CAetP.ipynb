


# Checking the environment
!echo $VIRTUAL_ENV
# and...
!which python
!which pip


!pip install "numpy<2"



# install spacy model
# https://spacy.io/usage
!python3 -m spacy download fr_core_news_sm
!pip install spacy seaborn
# ou python au lieu de python3


with open("frwiki_50.1000.conllu") as file: #file is a TextIOWrapper object.
    data1 = file.readlines() # liste de lignes
    #data = file.read() # chaÃ®ne de caractÃ¨res
    # for line in file:
    #     print(line)

data1[:20]
# data[-20:]





# Si le conllu est bien formatÃ©, on peut faire
n_sentences_A = data1.count("\n")
print(f"{n_sentences_A=}")


# Mais ce n'est pas du toujours le cas, alors...
n_sentences_B = 0
for line in data1:
    if line.startswith("# sent_id"):
        n_sentences_B += 1
print(f"{n_sentences_B=}")


# On peut sÃ©parer les phrases depuis le dÃ©but

def read_conllu(path):
    """
    Transformer le fichier conllu dans une liste de listes.
    Chaque liste est une phrase, chaque Ã©lÃ©ment est une ligne.
    """
    data = []
    with open(path, encoding="utf-8") as in_stream:
        next(in_stream)
        sent = []
        for line in in_stream:
            if line == "\n":
                data.append(sent)
                sent = []
            else:
                sent.append(line)
    return data

path = "frwiki_50.1000.conllu"
data2 = read_conllu(path)

for lst in data2[:1]:
    print(lst)

n_sentences_C = len(data2)
print(f"{n_sentences_C=}")


# On peut vÃ©rifier les rÃ©sultats, mÃªme si dans ce cas c'est inutile
assert  n_sentences_A == n_sentences_B and n_sentences_B == n_sentences_C





# D'autres informations importantes, telles que les mÃ©tadonnÃ©es et les caractÃ©ristiques, ne sont pas prises en compte.

def read_conllu(path) :
    """
    Transformer le fichier conllu dans une liste de liste.
    Chaque liste est une phrase, chaque token est un dictionnaire.
    Certains informations importantes, telles que les mÃ©tadonnÃ©es et les caractÃ©ristiques, ne sont pas prises en compte.
    """
    data = []

    with open(path, encoding="utf-8") as in_stream:
        sent = []
        for line in in_stream:
            line = line.strip()
            columns = line.split("\t")

            if columns[0].startswith("#"):
                continue
            if "-" in columns[0]:
                continue
            if columns[0]: # cÃ d, s'il n'est pas vide
                sent.append({
                    "idx" : columns[0],
                    "form" : columns[1],
                    "lemma" : columns[2],
                    "upos" : columns[3],
                    "head" : columns[6],
                    "deprel" : columns[7]})
            else:
                data.append(sent)
                sent = []

    return data

path = "frwiki_50.1000.conllu"
data = read_conllu(path)
data[0]





# Nombre de tokens
n_tokens = 0
for sentence in data:
    n_tokens += len(sentence)
print(n_tokens)


# ðŸš§ Quelle est la longueur moyenne d'une phrase dans le corpus ?
n_tokens / len(data)


# Combien y a-t-il de tokens diffÃ©rents ?
uniq_tokens = set()
for sentence in data:
    for tk in sentence:
        uniq_tokens.add(tk['form'])

print(f"{len(uniq_tokens)=}")


# Combien y a-t-il de lemmes diffÃ©rents ?
uniq_lemma = set()
for sentence in data:
    for lemma in sentence:
        uniq_lemma.add(lemma['lemma'])

print(f"{len(uniq_lemma)=}")


# Combien y a-t-il de lemmes diffÃ©rents ? RÃ©ponse avec une liste en comprÃ©hension  
uniq_lemmas = set([tk['lemma'] for sentence in data for tk in sentence])
print(f"{len(uniq_lemmas)=}")





# RÃ©cupÃ©rer la liste de tous tokens

lemmas = []
for sentence in data:
    for tk in sentence:
        if tk['upos'] != "PUNCT":
            lemmas.append(tk['lemma'])

# Avec une liste en comprÃ©hension  
#tokens = [tk['token'] for sentence in data for tk in sentence]

counter = dict()
for l in lemmas:
    if l in counter:
        counter[l] += 1
    else:
        counter[l] = 1

# On ordonne le dictionnaire et on obtient une liste de tuples ordonnÃ©s
sorted_lemmas = sorted(counter.items(), key=lambda item : item[1], reverse=True)
sorted_lemmas[:5]


# Avec la meilleure bibliothÃ¨que de Python
from collections import Counter

counter = Counter(lemmas)
sorted_lemmas = sorted(counter.items(), key=lambda item : item[1], reverse=True)
sorted_lemmas[:5]


!pip install seaborn


import seaborn as sns

freq = []
labels = []
for lemma in sorted_lemmas[:20]:
    freq.append(lemma[0])
    labels.append(lemma[1])

sns.barplot(x=freq, y=labels)


# ReprÃ©sentez graphiquement tous les noms et leur frÃ©quence avec un barplot
nouns = []
for sentence in data:
    for tk in sentence:
        if tk['upos'] == "NOUN":
            nouns.append(tk['lemma'])

counter_nouns = sorted(Counter(nouns).items(), key = lambda x: x[1], reverse=True)
freq = []
labels = []
for lemma in counter_nouns[:10]:
    freq.append(lemma[0])
    labels.append(lemma[1])

sns.barplot(x=freq, y=labels)


# ReprÃ©sentez la distribution de partie du discours avec un barplot
pos = []
for sentence in data:
    for tk in sentence:
        pos.append(tk['upos'])

counter_upos = sorted(Counter(pos).items(), key = lambda x: x[1], reverse=True)
freq = []
labels = []
for pos in counter_upos[:10]:
    freq.append(pos[0])
    labels.append(pos[1])

sns.barplot(x=freq, y=labels)


# Combien y a-t-il des adjectifs avant et aprÃ¨s leur nom?

adj_position = {"before" : 0, "after": 0}

# for sentence in data:
#     for token_i in sentence:
#         if token_i['upos'] == "NOUN":
#             for token_j in sentence:
                # Ã€ complÃ©ter, rÃ©ponse en bas...


adj_position = {"before" : 0, "after": 0}

for sentence in data:
    for token_i in sentence:
        if token_i['upos'] == "NOUN":
            for token_j in sentence:
                if token_j['head'] == token_i['idx'] and token_j['upos'] == "ADJ":
                    if int(token_i['idx']) < int(token_j['idx']):
                        adj_position['after'] += 1
                    else:
                        adj_position['before'] += 1

sns.barplot(adj_position)








# Une segmentation trÃ¨s naive
with open("conte.txt", encoding="utf-8") as file:
    text = file.read()
    
#sentences = [sent.strip() for sent in text.split(".")]
sentences = []
for sent in text.split('.'):
    sent_stripped = sent.strip()
    if sent_stripped:
        sentences.append(sent_stripped)
sentences[:3]


# Encore mieux avec spaCy
import spacy
from spacy import displacy
nlp = spacy.load("fr_core_news_sm")
# Je parse tout le text au mÃªme temps
doc = nlp(text)


list(doc.sents)


for sent in doc.sents:
    print(f"la racine est {sent.root.text}")
    for token in sent:
        print(token.text, token.pos_, token.morph, token.morph, token.lemma_, token.dep_)
    
    # pour trouver les enfants
    for child in token.children:
        print(token, child.text)

# upos = [(token.text, token.pos_) for token in doc]
# print(upos)


#dir(doc)


for token in list(doc.sents)[0][:3]:
    print(f"token : {token.text}\t deprel : {token.dep_}\t token_head : {token.head.text}")


# Trouver les verbes avec un sujet    
verbs = list()
for possible_subject in doc:
    if possible_subject.dep_ == "nsubj" and possible_subject.head.pos_ == "VERB":
        verbs.append(possible_subject.head)
print(verbs)


displacy.render(list(doc.sents)[0], style='dep')





# Analysez avec SpaCy les phrases du conte Le Joueur de flÃ»te de Hamelin. 
# a. Combient y a-t-il de mots ?
# b. Quelle est la phrase la plus longue ?
# c. Quelle est la longueur moyenne de phrases?
# d. Quel est le mot le plus longue ?
# e. Combient y a-t-il de verbes ?
# f. Plottez avec un barplot les dix mots les plus frÃ©quents.


# Encore Ã  vous...
# a. Quelle est la distribution de dÃ©pendances Ã  gauche et Ã  droite? Check spaCy API https://spacy.io/api/token
# b. Quelles sont les dÃ©pendances qui, dans au moins 90 % des cas, vont Ã  gauche ? 
# c. Quel est le nombre maximal de dÃ©pendants pour un nÅ“ud ?
# d. Y a-t-il des dÃ©pendances non projectives ?
